"""llama.cpp provider implementation.

This module contains llama.cpp server-specific API calls and token counting.
llama.cpp server provides an OpenAI-compatible API, so we use the openai library.
Provider-agnostic orchestration/prompt logic lives in `_llm.py`.
"""

from __future__ import annotations

from os import environ
from typing import ClassVar, Final

from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam
from tiktoken import Encoding, get_encoding

from ._llm import LLMTextResult, LLMUsage


_DEFAULT_LLAMACPP_HOST: Final[str] = "http://localhost:8080"


def _resolve_llamacpp_host(
    host: str | None,
    /,
) -> str:
    """Resolve the llama.cpp server host URL from arg, env, or default."""

    return host or environ.get("LLAMACPP_HOST") or _DEFAULT_LLAMACPP_HOST


def _get_encoding() -> Encoding:
    """Get a fallback encoding for token counting."""

    try:
        return get_encoding("cl100k_base")
    except Exception:
        return get_encoding("gpt2")


class LlamaCppProvider:
    """llama.cpp provider implementation for the LLM protocol.

    Uses the OpenAI-compatible API provided by llama.cpp server.
    """

    __slots__ = (
        "_host",
        "_client",
    )

    name: ClassVar[str] = "llamacpp"

    def __init__(
        self,
        /,
        *,
        host: str | None = None,
    ) -> None:
        self._host = _resolve_llamacpp_host(host)
        # llama.cpp server uses OpenAI-compatible API
        # api_key is not required but openai library needs a placeholder
        self._client = OpenAI(
            base_url=f"{self._host}/v1",
            api_key="llamacpp",  # Placeholder, llama.cpp doesn't require auth by default
        )

    def generate_text(
        self,
        /,
        *,
        model: str,
        instructions: str,
        user_text: str,
    ) -> LLMTextResult:
        """Generate text using llama.cpp server (OpenAI-compatible chat/completions API)."""

        messages: list[ChatCompletionMessageParam] = [
            {"role": "system", "content": instructions},
            {"role": "user", "content": user_text},
        ]

        try:
            resp = self._client.chat.completions.create(
                model=model,
                messages=messages,
            )
        except Exception as exc:
            raise RuntimeError(
                f"Failed to connect to llama.cpp server at {self._host}. "
                f"Make sure llama.cpp server is running: {exc}"
            ) from exc

        text: str = ""
        if resp.choices and len(resp.choices) > 0:
            choice = resp.choices[0]
            if choice.message and choice.message.content:
                text = choice.message.content.strip()

        if not text:
            raise RuntimeError("An empty response text was generated by the provider.")

        usage: LLMUsage | None = None
        if resp.usage is not None:
            usage = LLMUsage(
                prompt_tokens=resp.usage.prompt_tokens,
                completion_tokens=resp.usage.completion_tokens,
                total_tokens=resp.usage.total_tokens,
            )

        return LLMTextResult(
            text=text,
            response_id=resp.id,
            usage=usage,
        )

    def count_tokens(
        self,
        /,
        *,
        model: str,
        text: str,
    ) -> int:
        """Count tokens using llama.cpp's official token counting API."""

        try:
            # Use llama.cpp's official token counting endpoint via OpenAI client's internal HTTP client
            response = self._client.post(
                "/messages/count_tokens",
                body={
                    "model": model,
                    "messages": [
                        {"role": "user", "content": text}
                    ]
                },
                cast_to=dict,
            )
            return response.get("total", 0)
        except Exception:
            # Fallback to tiktoken approximation
            try:
                encoding = _get_encoding()
                return len(encoding.encode(text))
            except Exception:
                return len(text.split())
